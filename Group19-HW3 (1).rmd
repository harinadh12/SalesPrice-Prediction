---
title: "Homework#3"
author: "Group19"
date: "9/13/2021"
output:
  word_document: default
  html_document: default
---



```{r}
library(mlbench)
library(corrplot)
library(devtools)
library(ggbiplot)
library(tidyverse)
library(caret)
library(MASS)
library(lares)
library(HSAUR2)
library(outliers)

```


# Problem 1 -- Glass Data
```{r}
data(Glass)
Glass <- Glass[!duplicated(Glass),]
```

## Problem 1a --  Mathematics of PCA
**Problem(1a - i)**
```{r}
num_cols <- unlist(lapply(Glass,is.numeric))
Glass_num <-  Glass[,num_cols]
corMat <- cor(Glass_num)
corMat
```
`Computed the correlation matrix and assigned it to corMat`

**Problem(1a - ii)**
```{r}
eigen(corMat)
```
`Computed the eigen vectors and eigen values of correlation matrix corMat`

**Problem(1a - iii)**
```{r}
pca_pr <- prcomp(Glass_num, scale. = TRUE)
pca_pr
```

`Scaled and computed principal componenets of Glass Attributes`

**Problem(1a - iv)**
`The corresponding magnitude of elements in principal component vectors(PC1 - PC9) and eigen vectors(1-9) are same. However the direction of some vectors are reversed i.e., PC1, PC2, PC4&PC8 w.r.t eigen vectors 1,2,4&8 respectively.`



**Problem(1a - V)**
```{r}
pca <- as.data.frame(pca_pr$rotation)
if(round(sum(pca$PC1*pca$PC2),4)==0)
  print("Yes, PC1 and PC2 are Orthogonal")
```

## Problem 1b -- Application of PCA

**Problem(1b - i)**

```{r}
 corrplot(corMat,method='color')
```

**Problem(1b - ii)**

```{r}
class(pca_pr)
ggbiplot(pca_pr,groups = Glass$Type)
```


**Problem(1b - iii)**
`PC2 vs PC1 graph shows that Al,Na,Ba are positively related to PC1 and PC2. Si,K are negatively related to PC2 but positively influencing PC1, whereas Mg,Fe are negatively related to PC1,PC2 and Ca, RI are -vely related to PC1 but +vely related to PC2. As we increase in +ve direction of PC1,Si,K are strongly increased and as we increase in -ve direction of PC1 Fe is increased. PC1 accounts for around 28% variance and PC2 accounts for 23% variance.`



**Problem(1b - iv)**
```{r}
screeplot(prcomp(Glass_num,scale. = TRUE),type = "lines")
summary(pca_pr)
```
`Based on the observations we can reduce the dimensions to 4 instead of 9 and account for 80% of variation which we believe is a good amount of accounting variability of data in lower dimensional space`

## Problem 1c -- Application of LDA

**Problem(1c-i)**

```{r}
preprocess.param <- Glass %>% preProcess(method=c("center","scale"))
transformed <- preprocess.param %>% predict(Glass)
lda.model <- lda(Type~., data = transformed) 
predictions <- lda.model %>% predict(transformed)
table(Original=Glass$Type,Predicted=predictions$class)
print("Accuracy : ")
mean(predictions$class==transformed$Type)
```

**Problem(1c-ii)**
```{r}
#predictions$x
lda.model
```

`LD1 has higher power than other LD's and covers 82% of between class separatability.`


**Problem(1c-iii)**
```{r}
par(mar=c(1,1,1,1))
ldahist(data=predictions$x[,1],g=Glass$Type)
ldahist(data=predictions$x[,2],g=Glass$Type)
```

`Based on the histograms for LD1 and LD2, we can infer that LD1 was able to separate Types 1,2,3 from 5,6,7 better than LD2. However discrimintation at individual class level was not possible by both LD1 and LD2.`



```{r}
Glass$Type[1:20]
```

# Problem 2 -- Principal components for dimension reduction
```{r}
data(heptathlon)
data = heptathlon
```

**Problem(2a)**
```{r}
grubbs.test(data[,8]) # Grubbs's test
# Luana (PNG) is the competitor that is an outlier
grubbs.test(data[,1]) # outlier
grubbs.test(data[,2]) # outlier
grubbs.test(data[,3])
grubbs.test(data[,4])
grubbs.test(data[,5])# outlier
grubbs.test(data[,6])
grubbs.test(data[,7]) # outlier

# remove outlier
data <- data[-nrow(data),] # because we know that the outlier is the last one
data
```

**Problem(2b)**
```{r}
# transforming value

for (i in (1:nrow(data)))
{
  data[i,1] <- max(data[,1])- data[i,1] # transform max to "good" for hurdles
  data[i,4] <- max(data[,4]) - data[i,4] # tranform run200m
  data[i,7] <- max(data[,7]) - data[i,7] # tranform run200m
}
data
```

**Problem(2c)**
```{r}
# perform principle of analysis
Hpca <-prcomp(data, scale. = TRUE)
Hpca
```

**Problem(2d)**
```{r}
ggbiplot(Hpca)
# From the graph, PC1 accounted for 68.7% of variance, and PC2 accounted for 14.7% of variance
# hurdles, longjump, score and run200m strongly affecting the value of PC1 and javelin affects PC2.

```

**Problem(2e)**
```{r}

plot(Hpca$x[,1],data[,8],xlab = "PC1",ylab = "score") 
plot(Hpca$x[,2],data[,8],xlab = "PC2",ylab = "score")
```




# Problem 3 -- Housing data dimension reduction and exploration
```{r}
housingData <- read.csv("housingData.csv")
hd <- housingData %>%
  select_if(is.numeric) %>%
  dplyr::mutate(age = YrSold - YearBuilt,
                ageSinceRemodel = YrSold - YearRemodAdd,
                ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%
  dplyr::select(!c(Id,MSSubClass, LotFrontage, GarageYrBlt,
                   MiscVal, YrSold  , MoSold, YearBuilt,YearRemodAdd, MasVnrArea))
```

**PCA Analysis**
```{r}
hd_cov <- cov(hd)
hd <- subset(hd,select=-c(SalePrice))
hd_pcr <- prcomp(hd,scale. = TRUE)
screeplot(hd_pcr,type = "lines")
summary(hd_pcr)
```

`PC1 accounts for 23% of total variance in data and to account for 80% variance 12 atleast principal componenets are required.`

```{r}
#head(hd_pcr$x,10)
ggbiplot(hd_pcr,circle=TRUE,alpha=0,varname.size=1.75,varname.adjust=3)
```

`Lot Area, Fireplaces,Garage Cars,Garage Area,Open Porch SF  are affecting  PC1 positively and strongly whereas Pool area,Kitchen, bedroom abavrgr,X2nd Flr are affecting PC2 more.
+ Also, As the predicted variable is continuous(sales price) in this data set, biplot cant be grouped based on predictor class like Glass data set and require other form on analysis than PCA to better understand data set.`

```{r}
#cor(hd)
corrplot(corr = cor(hd),c1.offset=101)
```


```{r}
corr_cross(hd, 
            max_pvalue = 0.05, 
            top = 10 )
```

`10 most correlated variables and all of them are positively correlated using correlogram.`

